{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Debug finetune_clip",
            "type": "debugpy",
            "request": "launch",
            "module": "training.main",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1",
                "PYTHONPATH": "${workspaceFolder}"
            },
            "args": [
                "--model", "hf-hub:thaottn/OpenCLIP-resnet50-CC12M",
                "--train-data", "/ai/xxr/LLM/MMed-RAG/data/training/retriever/radiology/radiology_train.json",
                "--dataset-type", "radiology",
                "--img_root", "/ai/xxr/Datasets/Med/rad/iu_xray/images",
                "--batch-size", "1024",
                "--precision", "amp",
                "--workers", "4",
                "--lr", "0.0001",
                "--epochs", "360",
                "--val-data", "/ai/xxr/LLM/MMed-RAG/data/training/retriever/radiology/radiology_val.json",
                "--val-frequency", "10",
                "--report-to", "tensorboard",
                "--logs", "/ai/xxr/LLM/MMed-RAG/output/finetune_clip"
            ],
            "cwd": "${workspaceFolder}/train/open_clip/src",
            "console": "integratedTerminal"
        },
        {
            "name": "Python: Debug train_dpo_2stages",
            "type": "debugpy",
            "request": "launch",
            "module": "train.dpo.train_dpo_2stages",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1",
                "PYTHONPATH": "${workspaceFolder}"
            },
            "args": [
                "--model_name_or_path", "/ai/xxr/LLM/Models/llava-med-v1.5-mistral-7b",
                "--deepspeed", "./scripts/zero3.json",
                "--version", "v1",
                "--lora_enable", "True",
                "--lora_r", "128",
                "--lora_alpha", "256",
                "--mm_projector_lr", "2e-5",
                "--data_path", "/path/to/data_json",
                "--image_folder", "/path/to/img_folder",
                "--vision_tower", "openai/clip-vit-large-patch14-336",
                "--mm_projector_type", "mlp2x_gelu",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                "--image_aspect_ratio", "pad",
                "--group_by_modality_length", "True",
                "--bf16", "True",
                "--output_dir", "/path/to/output_checkpoint_saving_location",
                "--num_train_epochs", "3",
                "--per_device_train_batch_size", "1",
                "--per_device_eval_batch_size", "1",
                "--gradient_accumulation_steps", "1",
                "--evaluation_strategy", "no",
                "--save_strategy", "steps",
                "--save_steps", "200",
                "--save_total_limit", "1",
                "--learning_rate", "1e-7",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                "--report_to", "wandb",
                "--tf32", "True",
                "--model_max_length", "1024",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "4",
                "--lazy_preprocess", "True"
            ],
            "cwd": "${workspaceFolder}",
            "console": "integratedTerminal"
        }
    ]
}